// RUN: tf-mlir-translate -hlo-text-to-mlir-hlo %s -o - | FileCheck %s

HloModule tfcompile.7

// TODO(b/129422361) Potentially update when copy, reshape, and conv have actual
// implementations with attributes, etc.
// CHECK-LABEL: func @main(%arg0: tensor<256x32x32x6xf32>) -> tuple<tensor<256x30x30x16xf32>> {
ENTRY %tfcompile.7 {
  %arg0.1 = f32[256,32,32,6]{3,2,1,0} parameter(0), metadata={op_name="HLO_Args"}

  // CHECK-NEXT:   %0 = "xla_hlo.copy"(%arg0) {name = "copy.1"} : (tensor<256x32x32x6xf32>) -> tensor<256x32x32x6xf32>
  %copy.1 = f32[256,32,32,6]{2,1,3,0} copy(%arg0.1), metadata={op_name="HLO_Args"}

  // CHECK-NEXT:   %1 = "xla_hlo.reshape"(%0) {name = "reshape.2"} : (tensor<256x32x32x6xf32>) -> tensor<256x32x32x6xf32>
  %reshape.2 = f32[256,32,32,6]{2,1,3,0} reshape(%copy.1)

  // Note that double brackets "[[" have to be escaped as they denote variables
  // in FileCheck. The only way to do so is to drop into regex with "{{"
  // CHECK-NEXT:   %cst = constant  {name = "constant.3"} dense<{{\[\[\[\[}}5.000000e-01]], {{\[\[}}-6.000000e-01]]], {{\[\[\[}}3.000000e-01]], {{\[\[}}-1.000000e-01]]]]> : tensor<2x2x1x1xf32>
  %constant.3 = f32[2,2,1,1]{3,2,1,0} constant({{{{0.5}}, {{-0.6}}}, {{{0.3}}, {{-0.1}}}}), metadata={op_type="Conv2D" op_name="embedded_inference/conv_model/conv_0/Conv2D"}

  // CHECK-NEXT:   %2 = "xla_hlo.conv"(%1, %cst) {
  // CHECK-SAME:     batch_group_count = 1 : i64
  // CHECK-SAME:     dimension_numbers = {
  // CHECK-SAME:       input_batch_dimension = 0 : i64
  // CHECK-SAME:       input_feature_dimension = 3 : i64
  // CHECK-SAME:       input_spatial_dimensions = dense<[1, 2]> : tensor<2xi64>
  // CHECK-SAME:       kernel_input_feature_dimension = 2 : i64
  // CHECK-SAME:       kernel_output_feature_dimension = 3 : i64
  // CHECK-SAME:       kernel_spatial_dimensions = dense<[0, 1]> : tensor<2xi64>
  // CHECK-SAME:       output_batch_dimension = 0 : i64
  // CHECK-SAME:       output_feature_dimension = 3 : i64
  // CHECK-SAME:       output_spatial_dimensions = dense<[1, 2]> : tensor<2xi64>
  // CHECK-SAME:     }
  // CHECK-SAME:     feature_group_count = 1 : i64
  // CHECK-SAME:     lhs_dilations = dense<1> : tensor<2xi64>
  // CHECK-SAME:     name = "convolution.4"
  // CHECK-SAME:     padding = dense<{{\[\[}}44, 45], [60, 60]]> : tensor<2x2xi64>
  // CHECK-SAME:     precision_config = ["DEFAULT", "DEFAULT"]
  // CHECK-SAME:     rhs_dilations = dense<[2, 3]> : tensor<2xi64>
  // CHECK-SAME:     window_strides = dense<[4, 5]> : tensor<2xi64>
  // CHECK-SAME:   }
  // CHECK-SAME:   (tensor<256x32x32x6xf32>, tensor<2x2x1x1xf32>) -> tensor<256x30x30x16xf32>

  %convolution.4 = f32[256,30,30,16]{2,1,3,0} convolution(%reshape.2, %constant.3), window={size=3x3 stride=4x5 pad=44_45x60_60 rhs_dilate=2x3}, dim_labels=b01f_01io->b01f, metadata={op_type="Conv2D" op_name="embedded_inference/conv_model/conv_0/Conv2D"}

  // CHECK-NEXT:   %3 = "xla_hlo.reshape"(%2) {name = "reshape.5"} : (tensor<256x30x30x16xf32>) -> tensor<256x30x30x16xf32>
  %reshape.5 = f32[256,30,30,16]{3,2,1,0} reshape(%convolution.4), metadata={op_name="HLO_Retvals"}

  // CHECK-NEXT:   %4 = "xla_hlo.tuple"(%3) {name = "tuple.6"} : (tensor<256x30x30x16xf32>) -> tuple<tensor<256x30x30x16xf32>>
  // CHECK-NEXT:   return %4 : tuple<tensor<256x30x30x16xf32>>
  ROOT %tuple.6 = (f32[256,30,30,16]{3,2,1,0}) tuple(%reshape.5), metadata={op_name="HLO_Retvals"}
}
